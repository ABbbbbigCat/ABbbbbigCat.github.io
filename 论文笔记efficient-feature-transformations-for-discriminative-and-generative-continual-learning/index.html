<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning - 大猫的博客</title><meta name="Description" content="大猫的博客"><meta property="og:title" content="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning" />
<meta property="og:description" content="简介 本文发表于CVPR 2021，代码见此。 本文基于 architectural strategy，聚焦于提高效率（Efficient），即压缩网络模型，降低模型参数。文" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" /><meta property="og:image" content="https://capablecat.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-15T12:17:00&#43;08:00" />
<meta property="article:modified_time" content="2021-07-15T12:17:00&#43;08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://capablecat.github.io/logo.png"/>

<meta name="twitter:title" content="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning"/>
<meta name="twitter:description" content="简介 本文发表于CVPR 2021，代码见此。 本文基于 architectural strategy，聚焦于提高效率（Efficient），即压缩网络模型，降低模型参数。文"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" /><link rel="prev" href="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0continual-learning-via-bit-level-information-preserving/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/capablecat.github.io\/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning\/"
        },"genre": "posts","keywords": "incremental learning, architectural strategy","wordcount":  1613 ,
        "url": "https:\/\/capablecat.github.io\/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning\/","datePublished": "2021-07-15T12:17:00+08:00","dateModified": "2021-07-15T12:17:00+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "大猫"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="大猫的博客"></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/"> 主页 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="大猫的博客"></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/" title="">主页</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>大猫</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><i class="far fa-folder fa-fw"></i>论文笔记——深度学习</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-07-15">2021-07-15</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 1613 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 4 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="true">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#简介">简介</a></li>
    <li><a href="#方法">方法</a>
      <ul>
        <li><a href="#2d-convolution-layer">2D Convolution Layer</a></li>
        <li><a href="#fully-connected-layer">Fully Connected Layer</a></li>
        <li><a href="#task-prediction">Task Prediction</a></li>
      </ul>
    </li>
    <li><a href="#实验">实验</a></li>
    <li><a href="#参考">参考</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="简介">简介</h1>
<p>本文发表于<a href="https://arxiv.org/abs/2103.13558" target="_blank" rel="noopener noreffer">CVPR 2021</a>，代码<a href="https://github.com/vkverma01/EFT" target="_blank" rel="noopener noreffer">见此</a>。</p>
<p>本文基于 architectural strategy，聚焦于提高效率（Efficient），即压缩网络模型，降低模型参数。文章照例分析了现有的三种主流策略：rehearsal，regularization和archetectural。指出rehearsal受制于memory buffer，模型新旧任务间的bias严重；regularization则会限制模型的scalability且<strong>regularization通常猜想参数的重要性是独立的，而忽略了参数间的相关性</strong>（从底层角度，这是因为神经网络的黑盒特性）。而architectral可以从设计（design）上缓解灾难性遗忘，且根据新任务扩充网络可以保证模型的scalability（维护模型结构同时可能避免性能的unintentional degradation，因为许多模型诸如超参数设置等都是在工程上精心设计过的）。而architetural的的一个主要的缺陷是参数量大、性能消耗大，所以本文提出了一种新的特征变换方法，为其命名为 EFT （Efficient Feature Transformation）。</p>
<h1 id="方法">方法</h1>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2021/07/16/9LA8CFGHM2fek5n.png"
        data-srcset="https://i.loli.net/2021/07/16/9LA8CFGHM2fek5n.png, https://i.loli.net/2021/07/16/9LA8CFGHM2fek5n.png 1.5x, https://i.loli.net/2021/07/16/9LA8CFGHM2fek5n.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2021/07/16/9LA8CFGHM2fek5n.png"
        title="image-20210716023333621" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2021/07/16/S8zsOdmCNFMl7rZ.png"
        data-srcset="https://i.loli.net/2021/07/16/S8zsOdmCNFMl7rZ.png, https://i.loli.net/2021/07/16/S8zsOdmCNFMl7rZ.png 1.5x, https://i.loli.net/2021/07/16/S8zsOdmCNFMl7rZ.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2021/07/16/S8zsOdmCNFMl7rZ.png"
        title="image-20210715191528095" /></p>
<p>本文将参数分成了glbal parameter $\theta$和task-specific local parameter $\tau_t$，在训练任务t时，$\theta$和$\tau_{1:t-1}$保持不变，只有$\tau_t$被训练。Fig 1 center 是另一种参数化方法，它并行计算这些特征校准（图1，中间），使变换增加而不是合成。公式如下：</p>
<p>$$H^{'}=(W<em>I)\bigoplus(\tau^l_t</em>I)$$</p>
<p>其中$\bigoplus$是增加（addition），$\tau^l_t$是$l-layer$的 task-specific parameter。</p>
<h2 id="2d-convolution-layer">2D Convolution Layer</h2>
<p>简而言之，EFT设计了两类卷积核 $w^s \in 3<em>3</em>a$和$w^d\in1<em>1</em>b$分别用于计算特征的空间信息和深度信息。对于一个特征$F$，EFT将其分成了 $K/a$和$K/b$个group，每个group都有对应的$w^s$和$w^d$，将这些分割的特征通过concatenat，并将利用公式$H=H^s+\gamma H^d$就能空间信息和深度信息合成一个完整的feature map $H$了，它的结构与正常卷积的结构一致。公式中$\gamma \in {0,1}$用以指示是否使用 $w^d$。</p>
<h2 id="fully-connected-layer">Fully Connected Layer</h2>
<p>和卷积层一样，EFT也设计task-speicfic fully conneted layer，即通过另一个全连接层（由$E$参数化）将输出向量$F$转换为特定于任务的特征向量H。公式为$h=Ef$，为了节省计算开销，其中的$E$为对角矩阵，这让该公式实现了一个Hadamard product。</p>
<h2 id="task-prediction">Task Prediction</h2>
<p>对于CIL这种情况，本文采用了通过分类时的最大置信度预测（maximum confidence prediction）的来选择$\tau_t$。然而，由于典型的交叉熵训练目标倾向于在确定性神经网络中产生高置信度（即使对于分布外（OoD, out-of-distribution）样本），故softmax预测熵的简单直接测量通常表现不佳[1]。为了解决这个问题，本文提出了一种简单的正则化方式来最大化特征距离，以提高任务预测能力:</p>
<p>$$L_M=\sum_{i=1}^{t-1}max(\Delta-KL(P_t||Q_i),0)$$</p>
<p>其中 $P_t = N(μ_t,Σ_t) $和 $Q_i = N(μ_i,Σ_i)$ 是当前任务$t$ 和较早任务 $i &lt; t$ 的分布。 该正则化项帮助模型学习 $τ_t$ 的表示，使得当前任务数据 ($D_t$) 在特征空间中与由先前任务的参数 $τ&lt;t$ 编码的特征至少具有$∆$可分离性。</p>
<p>所以，最终的损失函数为：$L_{EFT} =L(yˆ,y)+λL_M$</p>
<h1 id="实验">实验</h1>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2021/07/16/7bjp8imKLcI5nsO.png"
        data-srcset="https://i.loli.net/2021/07/16/7bjp8imKLcI5nsO.png, https://i.loli.net/2021/07/16/7bjp8imKLcI5nsO.png 1.5x, https://i.loli.net/2021/07/16/7bjp8imKLcI5nsO.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2021/07/16/7bjp8imKLcI5nsO.png"
        title="image-20210716020002373" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2021/07/16/FPWvcZh3Gg7slbJ.png"
        data-srcset="https://i.loli.net/2021/07/16/FPWvcZh3Gg7slbJ.png, https://i.loli.net/2021/07/16/FPWvcZh3Gg7slbJ.png 1.5x, https://i.loli.net/2021/07/16/FPWvcZh3Gg7slbJ.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2021/07/16/FPWvcZh3Gg7slbJ.png"
        title="image-20210716020159293" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2021/07/16/96FNOAzxReJBH2o.png"
        data-srcset="https://i.loli.net/2021/07/16/96FNOAzxReJBH2o.png, https://i.loli.net/2021/07/16/96FNOAzxReJBH2o.png 1.5x, https://i.loli.net/2021/07/16/96FNOAzxReJBH2o.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2021/07/16/96FNOAzxReJBH2o.png"
        title="image-20210716020313194" /></p>
<blockquote>
<p>整体而言，本文在想法上基本遵循了之前的思想，并无太大创新，其内核和建筑化策略一致。但本文聚焦于计算效率问题，很大幅度的降低了参数增长，这是本文工作中最杰出的部分。至于建筑化的一些关键缺陷，比如<strong>推断时如何找到对应网络</strong>，本文依旧没有提出有效的解决方法。（Table 5 说明了$L_m$有作用，但效果很不明显。显然，基于置信度筛选还是非常容易翻车的）</p>
</blockquote>
<h1 id="参考">参考</h1>
<p>[1] <a href="http://proceedings.mlr.press/v70/guo17a.html" target="_blank" rel="noopener noreffer">Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein- berger. On calibration of modern neural networks, 2017. 4</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2021-07-15</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" data-title="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning" data-via="xxxx" data-hashtags="incremental learning,architectural strategy"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" data-hashtag="incremental learning"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" data-title="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" data-title="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" data-title="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Myspace" data-sharer="myspace" data-url="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" data-title="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a><a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" data-title="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning" data-description=""><i class="fab fa-blogger fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="https://capablecat.github.io/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0efficient-feature-transformations-for-discriminative-and-generative-continual-learning/" data-title="论文笔记——Efficient Feature Transformations for Discriminative and Generative Continual Learning"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/incremental-learning/">incremental learning</a>,&nbsp;<a href="/tags/architectural-strategy/">architectural strategy</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0continual-learning-via-bit-level-information-preserving/" class="prev" rel="prev" title="论文笔记——Continual Learning via Bit-Level Information Preserving"><i class="fas fa-angle-left fa-fw"></i>论文笔记——Continual Learning via Bit-Level Information Preserving</a></div>
</div>
<div id="comments"></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.83.1">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">大猫</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
